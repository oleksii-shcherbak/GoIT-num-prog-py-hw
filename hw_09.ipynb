{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkWnEJ0q0wJxBHlz7sSZuJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oleksii-shcherbak/GoIT-num-prog-py-hw/blob/main/hw_09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1"
      ],
      "metadata": {
        "id": "qBcueu889h7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Create the FrozenLake environment\n",
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True, render_mode=\"rgb_array\")\n",
        "\n",
        "def show_render(env):\n",
        "    \"\"\"\n",
        "    Function to visualize the environment state\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    img = env.render()\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title('FrozenLake Environment')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NUkgz0dN9rw5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2"
      ],
      "metadata": {
        "id": "zxgsufOx9ksq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_value_function(policy, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Compute the value function for a given policy using iterative policy evaluation.\n",
        "\n",
        "    Parameters:\n",
        "    policy: array of shape (n_states,) - policy to evaluate\n",
        "    gamma: float - discount factor\n",
        "\n",
        "    Returns:\n",
        "    value_table: array of shape (n_states,) - value function for the given policy\n",
        "    \"\"\"\n",
        "    # Initialize value table with zeros\n",
        "    value_table = np.zeros(env.observation_space.n)\n",
        "\n",
        "    # Set number of iterations and convergence threshold\n",
        "    no_of_iterations = 100000\n",
        "    threshold = 1e-10\n",
        "\n",
        "    for i in range(no_of_iterations):\n",
        "        # Copy current value table for comparison\n",
        "        updated_value_table = np.copy(value_table)\n",
        "\n",
        "        # For each state, compute value using the given policy\n",
        "        for state in range(env.observation_space.n):\n",
        "            # Get action from policy for current state\n",
        "            action = int(policy[state])\n",
        "\n",
        "            # Compute value for this state-action pair\n",
        "            value_table[state] = sum([\n",
        "                trans_prob * (reward_prob + gamma * updated_value_table[next_state])\n",
        "                for trans_prob, next_state, reward_prob, _ in env.P[state][action]\n",
        "            ])\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.sum(np.fabs(updated_value_table - value_table)) <= threshold:\n",
        "            print(f'Policy evaluation converged at iteration {i+1}')\n",
        "            break\n",
        "\n",
        "    return value_table"
      ],
      "metadata": {
        "id": "LiZ-eLaP9tX2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3"
      ],
      "metadata": {
        "id": "KI9yYRTJ9lnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(env, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Find the optimal policy using policy iteration algorithm.\n",
        "\n",
        "    Parameters:\n",
        "    env: gym environment\n",
        "    gamma: float - discount factor\n",
        "\n",
        "    Returns:\n",
        "    optimal_policy: array of shape (n_states,) - optimal policy\n",
        "    optimal_value_function: array of shape (n_states,) - optimal value function\n",
        "    \"\"\"\n",
        "    # Initialize random policy (all zeros)\n",
        "    old_policy = np.zeros(env.observation_space.n)\n",
        "    no_of_iterations = 1000\n",
        "\n",
        "    for i in range(no_of_iterations):\n",
        "        # Policy Evaluation: compute value function for current policy\n",
        "        new_value_function = compute_value_function(old_policy, gamma)\n",
        "\n",
        "        # Policy Improvement: extract new policy from value function\n",
        "        new_policy = extract_policy(new_value_function, gamma)\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.array_equal(old_policy, new_policy):\n",
        "            print(f'Policy iteration converged at step {i+1}')\n",
        "            break\n",
        "\n",
        "        # Update policy\n",
        "        old_policy = new_policy\n",
        "\n",
        "    return new_policy, new_value_function\n",
        "\n",
        "def extract_policy(value_table, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Extract policy from value function.\n",
        "\n",
        "    Parameters:\n",
        "    value_table: array of shape (n_states,) - value function\n",
        "    gamma: float - discount factor\n",
        "\n",
        "    Returns:\n",
        "    policy: array of shape (n_states,) - extracted policy\n",
        "    \"\"\"\n",
        "    # Initialize policy with zeros\n",
        "    policy = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for state in range(env.observation_space.n):\n",
        "        # Initialize Q-table for current state\n",
        "        Q_table = np.zeros(env.action_space.n)\n",
        "\n",
        "        # Compute Q-value for each action in current state\n",
        "        for action in range(env.action_space.n):\n",
        "            for trans_prob, next_state, reward_prob, _ in env.P[state][action]:\n",
        "                Q_table[action] += trans_prob * (reward_prob + gamma * value_table[next_state])\n",
        "\n",
        "        # Select action with maximum Q-value\n",
        "        policy[state] = np.argmax(Q_table)\n",
        "\n",
        "    return policy"
      ],
      "metadata": {
        "id": "2X6KzCSs9vSa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4"
      ],
      "metadata": {
        "id": "9aPq3M6_9mln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find optimal policy using policy iteration\n",
        "print(\"Starting Policy Iteration...\")\n",
        "optimal_policy, optimal_value_function = policy_iteration(env, gamma=0.9)\n",
        "\n",
        "print(\"\\nOptimal Value Function:\")\n",
        "print(optimal_value_function.reshape(4, 4))\n",
        "\n",
        "print(\"\\nOptimal Policy:\")\n",
        "policy_grid = optimal_policy.reshape(4, 4)\n",
        "print(policy_grid)\n",
        "\n",
        "# Convert policy numbers to direction symbols for better visualization\n",
        "action_symbols = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
        "print(\"\\nOptimal Policy (with direction symbols):\")\n",
        "for i in range(4):\n",
        "    row = \"\"\n",
        "    for j in range(4):\n",
        "        action = int(policy_grid[i, j])\n",
        "        row += action_symbols[action] + \" \"\n",
        "    print(row)\n",
        "\n",
        "# Simple policy demonstration without extensive testing\n",
        "print(\"\\nDemonstrating the optimal policy with a few steps:\")\n",
        "\n",
        "# Reset environment manually\n",
        "state = 0  # Start position\n",
        "print(f\"Starting at state: {state} (position [0,0])\")\n",
        "\n",
        "# Show first few optimal moves\n",
        "print(\"\\nOptimal actions from different states:\")\n",
        "for s in range(16):\n",
        "    if s not in [5, 7, 11, 12, 15]:  # Skip hole and goal states\n",
        "        action = int(optimal_policy[s])\n",
        "        action_name = ['←', '↓', '→', '↑'][action]\n",
        "        row, col = s // 4, s % 4\n",
        "        print(f\"State {s} (position [{row},{col}]): Action {action} ({action_name})\")\n",
        "\n",
        "# Show the environment layout\n",
        "print(\"\\nEnvironment Layout Reference:\")\n",
        "print(\"S = Start(0), F = Frozen, H = Hole, G = Goal(15)\")\n",
        "print(\"State numbers:\")\n",
        "print(\"[ 0  1  2  3 ]\")\n",
        "print(\"[ 4  H  6  H ]\")\n",
        "print(\"[ 8  9 10  H ]\")\n",
        "print(\"[ H 13 14  G ]\")\n",
        "\n",
        "print(\"\\nPolicy Interpretation:\")\n",
        "print(\"The policy shows the optimal direction to move from each state\")\n",
        "print(\"to maximize the probability of reaching the goal (state 15)\")\n",
        "print(\"while avoiding holes (states 5, 7, 11, 12)\")\n",
        "\n",
        "# Simple manual trace of one path\n",
        "print(\"\\nExample path following the policy:\")\n",
        "current_state = 0\n",
        "path = [current_state]\n",
        "visited_states = set()\n",
        "\n",
        "print(f\"Start: State {current_state}\")\n",
        "for step in range(10):  # Limit to 10 steps to avoid infinite loops\n",
        "    if current_state in visited_states:\n",
        "        print(\"Loop detected - stopping trace\")\n",
        "        break\n",
        "    visited_states.add(current_state)\n",
        "\n",
        "    if current_state == 15:  # Goal state\n",
        "        print(\"Reached goal!\")\n",
        "        break\n",
        "    if current_state in [5, 7, 11, 12]:  # Hole states\n",
        "        print(\"Fell in hole!\")\n",
        "        break\n",
        "\n",
        "    action = int(optimal_policy[current_state])\n",
        "    action_name = ['←', '↓', '→', '↑'][action]\n",
        "\n",
        "    # Simple deterministic next state calculation (ignoring slippery ice)\n",
        "    row, col = current_state // 4, current_state % 4\n",
        "    if action == 0 and col > 0:  # Left\n",
        "        next_state = current_state - 1\n",
        "    elif action == 1 and row < 3:  # Down\n",
        "        next_state = current_state + 4\n",
        "    elif action == 2 and col < 3:  # Right\n",
        "        next_state = current_state + 1\n",
        "    elif action == 3 and row > 0:  # Up\n",
        "        next_state = current_state - 4\n",
        "    else:\n",
        "        next_state = current_state  # Stay in place if move is invalid\n",
        "\n",
        "    print(f\"Step {step + 1}: State {current_state} → Action {action} ({action_name}) → State {next_state}\")\n",
        "    current_state = next_state\n",
        "    path.append(current_state)\n",
        "\n",
        "print(f\"\\nPath taken: {' → '.join(map(str, path))}\")\n",
        "\n",
        "print(\"\\nPolicy iteration completed successfully!\")\n",
        "print(\"The algorithm found an optimal policy that maximizes expected rewards.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AngWqjeg9xX_",
        "outputId": "d970ac31-77c3-4ae0-de45-434a12b654a3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Policy Iteration...\n",
            "Policy evaluation converged at iteration 1\n",
            "Policy evaluation converged at iteration 37\n",
            "Policy evaluation converged at iteration 111\n",
            "Policy evaluation converged at iteration 116\n",
            "Policy evaluation converged at iteration 155\n",
            "Policy evaluation converged at iteration 159\n",
            "Policy iteration converged at step 6\n",
            "\n",
            "Optimal Value Function:\n",
            "[[0.0688909  0.06141457 0.07440976 0.05580732]\n",
            " [0.09185454 0.         0.11220821 0.        ]\n",
            " [0.14543635 0.24749695 0.29961759 0.        ]\n",
            " [0.         0.3799359  0.63902015 0.        ]]\n",
            "\n",
            "Optimal Policy:\n",
            "[[0. 3. 0. 3.]\n",
            " [0. 0. 0. 0.]\n",
            " [3. 1. 0. 0.]\n",
            " [0. 2. 1. 0.]]\n",
            "\n",
            "Optimal Policy (with direction symbols):\n",
            "← ↑ ← ↑ \n",
            "← ← ← ← \n",
            "↑ ↓ ← ← \n",
            "← → ↓ ← \n",
            "\n",
            "Demonstrating the optimal policy with a few steps:\n",
            "Starting at state: 0 (position [0,0])\n",
            "\n",
            "Optimal actions from different states:\n",
            "State 0 (position [0,0]): Action 0 (←)\n",
            "State 1 (position [0,1]): Action 3 (↑)\n",
            "State 2 (position [0,2]): Action 0 (←)\n",
            "State 3 (position [0,3]): Action 3 (↑)\n",
            "State 4 (position [1,0]): Action 0 (←)\n",
            "State 6 (position [1,2]): Action 0 (←)\n",
            "State 8 (position [2,0]): Action 3 (↑)\n",
            "State 9 (position [2,1]): Action 1 (↓)\n",
            "State 10 (position [2,2]): Action 0 (←)\n",
            "State 13 (position [3,1]): Action 2 (→)\n",
            "State 14 (position [3,2]): Action 1 (↓)\n",
            "\n",
            "Environment Layout Reference:\n",
            "S = Start(0), F = Frozen, H = Hole, G = Goal(15)\n",
            "State numbers:\n",
            "[ 0  1  2  3 ]\n",
            "[ 4  H  6  H ]\n",
            "[ 8  9 10  H ]\n",
            "[ H 13 14  G ]\n",
            "\n",
            "Policy Interpretation:\n",
            "The policy shows the optimal direction to move from each state\n",
            "to maximize the probability of reaching the goal (state 15)\n",
            "while avoiding holes (states 5, 7, 11, 12)\n",
            "\n",
            "Example path following the policy:\n",
            "Start: State 0\n",
            "Step 1: State 0 → Action 0 (←) → State 0\n",
            "Loop detected - stopping trace\n",
            "\n",
            "Path taken: 0 → 0\n",
            "\n",
            "Policy iteration completed successfully!\n",
            "The algorithm found an optimal policy that maximizes expected rewards.\n"
          ]
        }
      ]
    }
  ]
}